import os
import json
from glob import glob
from typing import List, Dict, Any
from collections import Counter
import matplotlib.pyplot as plt
import koreanize_matplotlib     # ğŸ‘ˆ 1. í•œê¸€ í°íŠ¸ import
from wordcloud import WordCloud
from multiprocessing import Pool # ğŸ‘ˆ 2. ìµœì í™”(ë³‘ë ¬ ì²˜ë¦¬)
import os                      # ğŸ‘ˆ 2. ìµœì í™”(CPU ì½”ì–´ ìˆ˜)
from tqdm import tqdm          # ğŸ‘ˆ 3. ì§„í–‰ë¥  í‘œì‹œ
from transformers import pipeline  # Hugging Face Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€

# --- (1/5) Okt ë¡œë” ë° í† í¬ë‚˜ì´ì € ì •ì˜ ---

# Okt ê°ì²´ëŠ” ë¡œë”© ì‹œê°„ì´ ê±¸ë¦¬ë¯€ë¡œ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì‹œ í•œ ë²ˆë§Œ ìƒì„±í•©ë‹ˆë‹¤.
try:
    from konlpy.tag import Okt
    okt = Okt()
except Exception as e:
    print(f"KoNLPy(Okt) ë¡œë”© ì‹¤íŒ¨. 1. 'pip install konlpy' 2. Java ì„¤ì¹˜ ë° JAVA_HOME í™˜ê²½ë³€ìˆ˜ í™•ì¸ í•„ìš”. ì˜¤ë¥˜: {e}")
    exit()

# [Before] ë² ì´ìŠ¤ë¼ì¸ì˜ simple_tokenize í•¨ìˆ˜
def simple_tokenize(s: str) -> List[str]:
    s = (s or "")
    s = s.replace("##", " ").replace(",", " ").replace("(", " ").replace(")", " ")
    s = s.replace(":", " ").replace("?", " ").replace("!", " ").replace("Â·", " ")
    return [t for t in s.strip().split() if t]

# [After] ìš°ë¦¬ê°€ ê°œì„ í•œ new_tokenize í•¨ìˆ˜
# â— 1. ì œê±°í•  í’ˆì‚¬ íƒœê·¸ ì •ì˜ (ë¶ˆìš©ì–´ íƒœê·¸)
STOP_TAGS = ['Josa', 'Punctuation', 'Suffix', 'Eomi', 'Verb']

# â— 2. ì œê±°í•  ë‹¨ì–´ ì •ì˜ (ë¶ˆìš©ì–´)
STOP_WORDS = ['ìˆë‹¤', 'í•˜ë‹¤', 'ê°™ë‹¤', 'ì–´ë””', 'ëŒ€í•´', 'ì•Œë¦¬ë‹¤', 'ë³´ì´ë‹¤', 'ì•Œë‹¤'] 

def new_tokenize(s: str) -> list[str]:
    """Oktë¥¼ ì‚¬ìš©í•´ ë¬¸ì¥ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ(ëª…ì‚¬ ë“±)ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    s = (s or "")
    if not s:
        return []
    try:
        pos_result = okt.pos(s, norm=True, stem=True)
    except Exception:
        return []

    keywords = []
    for word, tag in pos_result:
        if tag in STOP_TAGS or word in STOP_WORDS:
            continue
        if len(word) > 1: # í•œ ê¸€ì ë‹¨ì–´ ì œê±° (ì˜ˆ: 'ê²½', 'ì œ')
            keywords.append(word)
    return keywords

# --- (2/5) JSON ë°ì´í„° ë¡œë”© í•¨ìˆ˜ ---

def find_jsons(json_dir: str) -> List[str]:
    """ì§€ì •ëœ ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  .json íŒŒì¼ ê²½ë¡œë¥¼ ì°¾ìŠµë‹ˆë‹¤. (í•˜ìœ„ í´ë” í¬í•¨)"""
    if os.path.isdir(json_dir):
        # recursive=Trueë¡œ í•˜ìœ„ í´ë”ì˜ ëª¨ë“  jsonì„ ê²€ìƒ‰
        return sorted(glob(os.path.join(json_dir, "**", "*.json"), recursive=True))
    raise FileNotFoundError(f"json_dir not found: {json_dir}")

def read_json(path: str) -> Dict[str, Any]:
    """JSON íŒŒì¼ì„ ì½ì–´ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

def summarize_text(text: str, max_length: int = 50, min_length: int = 10) -> str:
    """
    Hugging Face Transformersì˜ ìš”ì•½ íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤.
    """
    summarizer = pipeline("summarization", model="sshleifer/distilbart-cnn-12-6")
    try:
        summary = summarizer(text, max_length=max_length, min_length=min_length, do_sample=False)
        return summary[0]['summary_text']
    except Exception as e:
        print(f"[ìš”ì•½ ì˜¤ë¥˜] {e}")
        return text  # ìš”ì•½ ì‹¤íŒ¨ ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ ë°˜í™˜

def extract_all_instructions_and_answers(json_dir_list: List[str]) -> List[Dict[str, str]]:
    """
    ì£¼ì–´ì§„ *í´ë” ë¦¬ìŠ¤íŠ¸*ì˜ ëª¨ë“  JSON íŒŒì¼ì„ ìˆœíšŒí•˜ë©°
    'visual_instruction'ê³¼ 'visual_answer'ë¥¼ ì¶”ì¶œí•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    all_data = []
    print("=== Instruction ë° Answer ì¶”ì¶œ ì‹œì‘ ===")
    for folder_path in json_dir_list:
        print(f"--- '{folder_path}' í´ë” ê²€ìƒ‰ ì¤‘... ---")
        try:
            json_files = find_jsons(folder_path)
            print(f"ì´ {len(json_files)}ê°œì˜ JSON íŒŒì¼ ë°œê²¬.")
        except FileNotFoundError as e:
            print(f"[ì˜¤ë¥˜] {e}")
            continue

        # tqdmìœ¼ë¡œ ê°œë³„ íŒŒì¼ ì²˜ë¦¬ ì§„í–‰ë¥  í‘œì‹œ
        for json_path in tqdm(json_files, desc=f"  -> {os.path.basename(folder_path)} ì²˜ë¦¬ ì¤‘", leave=False):
            try:
                data = read_json(json_path)
                annotations = data.get("learning_data_info", {}).get("annotation", [])
                if not annotations:
                    continue 

                for ann in annotations:
                    instruction = ann.get("visual_instruction")
                    answer = ann.get("visual_answer")
                    if instruction:
                        summarized_answer = summarize_text(answer) if answer else ""
                        all_data.append({
                            "instruction": instruction,
                            "answer": summarized_answer
                        })
            except Exception:
                pass  # ê°œë³„ íŒŒì¼ ì˜¤ë¥˜ëŠ” ì¡°ìš©íˆ ê±´ë„ˆë›°ê¸°
    return all_data

# --- (3/5) ì‹œê°í™” í•¨ìˆ˜ (ì›Œë“œ í´ë¼ìš°ë“œ) ---

def plot_word_cloud(tokens: List[str], save_path: str = "wordcloud_after.png"):
    """
    (After ê¸°ì¤€) ì¶”ì¶œëœ í† í° ë¦¬ìŠ¤íŠ¸ë¡œ ì›Œë“œ í´ë¼ìš°ë“œë¥¼ ìƒì„±í•˜ê³  íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    """
    print(f"\n--- ì›Œë“œ í´ë¼ìš°ë“œ ìƒì„± ì¤‘... ({save_path}) ---")
    if not tokens:
        print("[ê²½ê³ ] ì›Œë“œ í´ë¼ìš°ë“œë¥¼ ë§Œë“¤ í† í°ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    counts = Counter(tokens)
    # í°íŠ¸ ê²½ë¡œë¥¼ ìˆ˜ë™ ì§€ì •(malgun.ttf)í•˜ê±°ë‚˜, koreanize_matplotlib ê²ƒì„ ì‚¬ìš©
    font_path = koreanize_matplotlib.get_font_path()
    # font_path = "C:/Windows/Fonts/malgun.ttf" # ìœˆë„ìš° ì‚¬ìš©ì ìˆ˜ë™ ì§€ì • ì˜ˆì‹œ
    
    wc = WordCloud(
        font_path=font_path,
        width=800,
        height=600,
        background_color="white",
        max_words=100
    )
    wc.generate_from_frequencies(counts)
    
    plt.figure(figsize=(10, 8))
    plt.imshow(wc, interpolation='bilinear')
    plt.axis('off')
    plt.savefig(save_path)
    print(f"âœ… ì›Œë“œ í´ë¼ìš°ë“œê°€ '{save_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    plt.close()

# --- (4/5) ì‹œê°í™” í•¨ìˆ˜ (Before-After ë¹„êµ ì°¨íŠ¸) ---

def plot_before_after_chart(
    before_tokens: List[str], 
    after_tokens: List[str], 
    n: int = 20, 
    save_path: str = "keywords_compare.png"
):
    """
    [Before]ì™€ [After]ì˜ ìƒìœ„ Nê°œ í‚¤ì›Œë“œë¥¼ ë‚˜ë€íˆ ë°” ì°¨íŠ¸ë¡œ ê·¸ë ¤ ì €ì¥í•©ë‹ˆë‹¤.
    """
    print(f"\n--- Before-After ìƒìœ„ {n}ê°œ í‚¤ì›Œë“œ ë¹„êµ ì°¨íŠ¸ ìƒì„± ì¤‘... ---")
    if not before_tokens or not after_tokens:
        print("[ê²½ê³ ] ë¹„êµ ì°¨íŠ¸ë¥¼ ë§Œë“¤ í† í°ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 1. (Before) ìƒìœ„ Nê°œ í‚¤ì›Œë“œ ì¶”ì¶œ
    before_common = Counter(before_tokens).most_common(n)
    before_common.reverse()
    before_labels = [item[0] for item in before_common]
    before_freqs = [item[1] for item in before_common]

    # 2. (After) ìƒìœ„ Nê°œ í‚¤ì›Œë“œ ì¶”ì¶œ
    after_common = Counter(after_tokens).most_common(n)
    after_common.reverse()
    after_labels = [item[0] for item in after_common]
    after_freqs = [item[1] for item in after_common]

    # 3. 1x2 (ê°€ë¡œ 2ì¹¸) ì„œë¸Œí”Œë¡¯ ìƒì„±
    fig, axes = plt.subplots(1, 2, figsize=(20, 10)) # 1ì¤„ 2ì¹¸
    
    # 4. [Before] ì°¨íŠ¸ ê·¸ë¦¬ê¸° (ì™¼ìª½: axes[0])
    axes[0].barh(before_labels, before_freqs, color='royalblue')
    axes[0].set_title('Before: Baseline (simple_tokenize)', fontsize=16)
    axes[0].set_xlabel('ë¹ˆë„ìˆ˜')
    axes[0].set_ylabel('í† í°')
    axes[0].grid(axis='x', linestyle='--', alpha=0.7)

    # 5. [After] ì°¨íŠ¸ ê·¸ë¦¬ê¸° (ì˜¤ë¥¸ìª½: axes[1])
    axes[1].barh(after_labels, after_freqs, color='darkviolet')
    axes[1].set_title('After: Improved (KoNLPy + Stopwords)', fontsize=16)
    axes[1].set_xlabel('ë¹ˆë„ìˆ˜')
    axes[1].set_ylabel('í•µì‹¬ í‚¤ì›Œë“œ')
    axes[1].grid(axis='x', linestyle='--', alpha=0.7)

    # 6. ì „ì²´ ì œëª© ë° ë ˆì´ì•„ì›ƒ ì„¤ì •
    fig.suptitle(f'Instruction í‚¤ì›Œë“œ ë¶„ì„ (Top {n}): Before vs After', fontsize=20, y=1.03)
    plt.tight_layout()
    
    # 7. íŒŒì¼ë¡œ ì €ì¥
    plt.savefig(save_path)
    print(f"âœ… ë¹„êµ ì°¨íŠ¸ê°€ '{save_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    plt.close()

# --- (5/5) ìŠ¤í¬ë¦½íŠ¸ ë©”ì¸ ì‹¤í–‰ ---
if __name__ == "__main__":
    
    # â—â— ì—¬ê¸°ì— ë¶„ì„í•  ëª¨ë“  JSON í´ë” ê²½ë¡œë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë„£ì–´ì£¼ì„¸ìš”.
    JSON_DIR_LIST = [
        "C:/Users/jrne/Desktop/train_valid/train/press_json",
        "C:/Users/jrne/Desktop/train_valid/train/report_json"
        # "ë˜ ë‹¤ë¥¸ í´ë” ê²½ë¡œê°€ ìˆë‹¤ë©´ ì—¬ê¸°ì— ì¶”ê°€..."
    ]
    
    # 1. ëª¨ë“  instruction ë° answer ì¶”ì¶œ
    all_data = extract_all_instructions_and_answers(JSON_DIR_LIST)
    
    if not all_data:
        print("="*40)
        print("âŒ ë¶„ì„í•  instructionê³¼ answerë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. JSON_DIR_LIST ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        print("="*40)
    else:
        print("\n" + "="*40)
        print(f"âœ… ì´ {len(all_data)}ê°œì˜ instruction ë° answer ìˆ˜ì§‘ ì™„ë£Œ. í† í°í™” ì‹œì‘...")
        print("="*40)

        # 2. [Before]ì™€ [After] í† í° ë¦¬ìŠ¤íŠ¸ë¥¼ *ë³‘ë ¬*ë¡œ ìƒì„±
        
        # CPU ì½”ì–´ ìˆ˜ í™•ì¸ (ìµœëŒ€ 8ê°œ or í˜„ì¬ ì½”ì–´ ìˆ˜)
        num_cores = min(os.cpu_count() or 4, 8) 
        print(f"--- {num_cores}ê°œì˜ CPU ì½”ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘ ---")

        # [Before] í† í°í™” (simple_tokenizeëŠ” ë§¤ìš° ë¹ ë¥´ë¯€ë¡œ êµ³ì´ ë³‘ë ¬ì²˜ë¦¬ ì•ˆ í•¨)
        before_tokens = []
        for data in all_data:
            before_tokens.extend(simple_tokenize(data["instruction"] + " " + data["answer"]))
        print("--- [Before] í† í°í™” ì™„ë£Œ (fast) ---")
        
        # [After] í† í°í™” (new_tokenizeëŠ” ë§¤ìš° ëŠë¦¬ë¯€ë¡œ ë³‘ë ¬ ì²˜ë¦¬)
        after_tokens_list = []  # [[t1, t2], [t3, t4], ...] í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸
        
        # 3. Pool ê°ì²´ë¥¼ ìƒì„±í•˜ì—¬ new_tokenize í•¨ìˆ˜ë¥¼ ë³‘ë ¬ ì‹¤í–‰
        with Pool(processes=num_cores) as pool:
            # pool.imap: í•¨ìˆ˜ë¥¼ ë°ì´í„°ì— ë§¤í•‘. tqdmìœ¼ë¡œ ì§„í–‰ë¥  í‘œì‹œ
            after_tokens_list = list(tqdm(
                pool.imap(new_tokenize, [data["instruction"] + " " + data["answer"] for data in all_data], chunksize=100),  # ğŸ‘ˆ chunksize ì¶”ê°€
                total=len(all_data),
                desc="[After] í† í°í™” ì¤‘"
            ))
        
        # 4. ë³‘ë ¬ ì²˜ë¦¬ëœ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ í¼ì¹˜ê¸° (Flatten)
        after_tokens = [token for sublist in after_tokens_list for token in sublist]
        
        print("--- ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ. ì‹œê°í™” ì‹œì‘... ---")

        # 5. ì‹œê°í™” í•¨ìˆ˜ í˜¸ì¶œ
        
        # (ì‹œê°í™” 1) Before-After ë¹„êµ ë°” ì°¨íŠ¸
        plot_before_after_chart(
            before_tokens, 
            after_tokens, 
            n=20, 
            save_path="keywords_compare.png"
        )
        
        # (ì‹œê°í™” 2) After ê¸°ì¤€ ì›Œë“œ í´ë¼ìš°ë“œ
        plot_word_cloud(after_tokens, save_path="wordcloud_after.png")

        print("\n" + "="*40)
        print("âœ¨ ëª¨ë“  ì‹œê°í™” ìë£Œ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. (png íŒŒì¼ í™•ì¸)")
        print("="*40)