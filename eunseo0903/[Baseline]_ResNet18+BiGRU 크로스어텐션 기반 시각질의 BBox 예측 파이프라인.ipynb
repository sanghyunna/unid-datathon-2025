{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58b1a0e2",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94c81f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import argparse\n",
    "import zipfile\n",
    "from glob import glob\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bcae17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchvision 백본 사용 가능 여부 체크\n",
    "try:\n",
    "    from torchvision.models import resnet18, ResNet18_Weights\n",
    "    _BACKBONE_OK = True\n",
    "except Exception:\n",
    "    _BACKBONE_OK = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805720ff",
   "metadata": {},
   "source": [
    "# Hyperparameter Setting (CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43161f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    # Core\n",
    "    IMG_SIZE: int = 512\n",
    "    EPOCHS: int = 10\n",
    "    LEARNING_RATE: float = 1e-4\n",
    "    BATCH_SIZE: int = 8\n",
    "    SEED: int = 42\n",
    "    DIM: int = 256\n",
    "    NUM_WORKERS: int = 2\n",
    "    NO_PRETRAIN: bool = False  # True → disable ImageNet weights\n",
    "\n",
    "    # Paths (override by CLI if desired)\n",
    "    JSON_DIR: str = \"./data/json\"\n",
    "    JPG_DIR: str = None\n",
    "    CKPT_PATH: str = \"./outputs/ckpt/cross_attn_vlm.pth\"\n",
    "    EVAL_CSV: str = \"./outputs/preds/eval_pred.csv\"\n",
    "    PRED_CSV: str = \"./outputs/preds/test_pred.csv\"\n",
    "    SUBMISSION_ZIP: str = \"./outputs/submission.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe76ffc",
   "metadata": {},
   "source": [
    "# Fixed RandomSeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a199700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5daac8",
   "metadata": {},
   "source": [
    "# Data Pre-processing (helpers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b57809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_jsons(json_dir: str) -> List[str]:\n",
    "    if os.path.isdir(json_dir):\n",
    "        return sorted(glob(os.path.join(json_dir, \"*.json\")))\n",
    "    raise FileNotFoundError(f\"json_dir not found: {json_dir}\")\n",
    "\n",
    "\n",
    "def read_json(path: str) -> Dict[str, Any]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def get_image_path(json_path: str, data: Dict[str, Any], jpg_dir: str = None) -> str:\n",
    "    # Prefer explicit mapping via source_data_name_jpg\n",
    "    src = data.get(\"source_data_info\", {})\n",
    "    jpg_name = src.get(\"source_data_name_jpg\", None)\n",
    "    if jpg_dir and jpg_name:\n",
    "        path = os.path.join(jpg_dir, jpg_name)\n",
    "        if os.path.exists(path):\n",
    "            return path\n",
    "    # Fallback: .../json/... -> .../jpg/...\n",
    "    if jpg_name:\n",
    "        maybe = json_path.replace(os.sep + \"json\" + os.sep, os.sep + \"jpg\" + os.sep)\n",
    "        maybe = os.path.join(os.path.dirname(maybe), jpg_name) if os.path.isdir(os.path.dirname(maybe)) else maybe\n",
    "        if os.path.exists(maybe):\n",
    "            return maybe\n",
    "    # Last resort: same dir, MI3 -> MI2.jpg\n",
    "    base = os.path.splitext(os.path.basename(json_path))[0]\n",
    "    sibling = os.path.join(os.path.dirname(json_path), base.replace(\"MI3\", \"MI2\") + \".jpg\")\n",
    "    if os.path.exists(sibling):\n",
    "        return sibling\n",
    "    raise FileNotFoundError(f\"Could not resolve JPG for {json_path} (jpg_dir={jpg_dir})\")\n",
    "\n",
    "\n",
    "def simple_tokenize(s: str) -> List[str]:\n",
    "    s = (s or \"\")\n",
    "    s = s.replace(\"##\", \" \").replace(\",\", \" \").replace(\"(\", \" \").replace(\")\", \" \")\n",
    "    s = s.replace(\":\", \" \").replace(\"?\", \" \").replace(\"!\", \" \").replace(\"·\", \" \")\n",
    "    return [t for t in s.strip().split() if t]\n",
    "\n",
    "\n",
    "def is_visual_ann(a: dict) -> bool:\n",
    "    \"\"\"Use only visual elements that have a query: V* class_id or table/chart-related class_name, and non-empty instruction.\"\"\"\n",
    "    cid = str(a.get(\"class_id\", \"\") or \"\")\n",
    "    cname = str(a.get(\"class_name\", \"\") or \"\")\n",
    "    has_q = bool(str(a.get(\"visual_instruction\", \"\") or \"\").strip())\n",
    "    looks_visual = cid.startswith(\"V\") or any(k in cname for k in [\"표\", \"차트\", \"그래프\", \"chart\", \"table\"])\n",
    "    return has_q and looks_visual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e4faee",
   "metadata": {},
   "source": [
    "# CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1f2495",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, min_freq: int = 1):\n",
    "        self.min_freq = min_freq\n",
    "        self.freq: Dict[str, int] = {}\n",
    "        self.itos: List[str] = [\"<pad>\", \"<unk>\"]\n",
    "        self.stoi: Dict[str, int] = {tok: i for i, tok in enumerate(self.itos)}\n",
    "\n",
    "    def build(self, texts: List[str]):\n",
    "        for s in texts:\n",
    "            for tok in simple_tokenize(s):\n",
    "                self.freq[tok] = self.freq.get(tok, 0) + 1\n",
    "        for tok, f in sorted(self.freq.items(), key=lambda x: (-x[1], x[0])):\n",
    "            if f >= self.min_freq and tok not in self.stoi:\n",
    "                self.stoi[tok] = len(self.itos)\n",
    "                self.itos.append(tok)\n",
    "\n",
    "    def encode(self, s: str, max_len: int = 40) -> List[int]:\n",
    "        toks = simple_tokenize(s)[:max_len]\n",
    "        if not toks:\n",
    "            return [1]  # ensure length>=1 with <unk>\n",
    "        return [self.stoi.get(t, 1) for t in toks]\n",
    "\n",
    "\n",
    "class UniDSet(Dataset):\n",
    "    def __init__(self, json_files: List[str], jpg_dir: str = None, vocab: Vocab = None,\n",
    "                 build_vocab: bool = False, resize_to: Tuple[int, int] = (CFG.IMG_SIZE, CFG.IMG_SIZE)):\n",
    "        self.items = []\n",
    "        for jf in json_files:\n",
    "            data = read_json(jf)\n",
    "            ann = data.get(\"learning_data_info\", {}).get(\"annotation\", [])\n",
    "            img_path = get_image_path(jf, data, jpg_dir=jpg_dir)\n",
    "            for a in ann:\n",
    "                if not is_visual_ann(a):\n",
    "                    continue\n",
    "                qid = a.get(\"instance_id\", \"\")\n",
    "                qtxt = str(a.get(\"visual_instruction\", \"\")).strip()\n",
    "                bbox = a.get(\"bounding_box\", None)  # train/val has bbox; test may be None\n",
    "                cname = a.get(\"class_name\", \"\")\n",
    "                self.items.append({\n",
    "                    \"json\": jf, \"img\": img_path,\n",
    "                    \"query_id\": qid, \"query\": qtxt,\n",
    "                    \"bbox\": bbox, \"class_name\": cname,\n",
    "                })\n",
    "\n",
    "        self.vocab = vocab if vocab is not None else Vocab(min_freq=1)\n",
    "        if build_vocab:\n",
    "            self.vocab.build([it[\"query\"] for it in self.items])\n",
    "\n",
    "        self.resize_to = resize_to\n",
    "        if _BACKBONE_OK:\n",
    "            from torchvision import transforms as T\n",
    "            self.tf = T.Compose([T.Resize(resize_to), T.ToTensor()])\n",
    "        else:\n",
    "            self.tf = None  # will manually convert\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    @staticmethod\n",
    "    def _pil_to_tensor(img: Image.Image) -> torch.Tensor:\n",
    "        arr = np.array(img).astype(np.float32) / 255.0\n",
    "        if arr.ndim == 2:\n",
    "            arr = np.stack([arr, arr, arr], axis=-1)\n",
    "        arr = np.transpose(arr, (2, 0, 1))\n",
    "        return torch.from_numpy(arr)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        it = self.items[idx]\n",
    "        img = Image.open(it[\"img\"]).convert(\"RGB\")\n",
    "        W, H = img.size\n",
    "        if self.tf is not None:\n",
    "            img_t = self.tf(img)\n",
    "        else:\n",
    "            img = img.resize(self.resize_to, Image.BILINEAR)\n",
    "            img_t = self._pil_to_tensor(img)\n",
    "\n",
    "        ids = self.vocab.encode(it[\"query\"], max_len=40)\n",
    "        length = max(1, len(ids))  # safety: ensure >=1\n",
    "\n",
    "        sample: Dict[str, Any] = {\n",
    "            \"image\": img_t,\n",
    "            \"query_ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            \"length\": torch.tensor(length, dtype=torch.long),\n",
    "            \"query_text\": it[\"query\"],\n",
    "            \"query_id\": it[\"query_id\"],\n",
    "            \"orig_size\": (W, H),\n",
    "            \"class_name\": it[\"class_name\"],\n",
    "        }\n",
    "        if it[\"bbox\"] is not None and isinstance(it[\"bbox\"], (list, tuple)) and len(it[\"bbox\"]) == 4:\n",
    "            x, y, w, h = it[\"bbox\"]\n",
    "            cx = (x + w / 2.0) / W\n",
    "            cy = (y + h / 2.0) / H\n",
    "            nw = w / W\n",
    "            nh = h / H\n",
    "            target = torch.tensor([cx, cy, nw, nh], dtype=torch.float32)\n",
    "        else:\n",
    "            target = None\n",
    "        sample[\"target\"] = target\n",
    "        return sample\n",
    "\n",
    "\n",
    "def collate_fn(batch: List[Dict[str, Any]]):\n",
    "    # pad variable-length queries\n",
    "    max_len = max(max(1, int(b[\"length\"])) for b in batch)\n",
    "    B = len(batch)\n",
    "    ids = torch.zeros(B, max_len, dtype=torch.long)\n",
    "    lens = torch.zeros(B, dtype=torch.long)\n",
    "    imgs = torch.stack([b[\"image\"] for b in batch], dim=0)\n",
    "    targets = []\n",
    "    meta = []\n",
    "    for i, b in enumerate(batch):\n",
    "        l = max(1, int(b[\"length\"]))\n",
    "        ids[i, :l] = b[\"query_ids\"][:l]\n",
    "        lens[i] = l\n",
    "        targets.append(b[\"target\"])\n",
    "        meta.append({\n",
    "            \"query_id\": b[\"query_id\"],\n",
    "            \"query_text\": b[\"query_text\"],\n",
    "            \"orig_size\": b[\"orig_size\"],\n",
    "            \"class_name\": b[\"class_name\"],\n",
    "        })\n",
    "    return imgs, ids, lens, targets, meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fe50b1",
   "metadata": {},
   "source": [
    "# Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0162cbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_dim: int = CFG.DIM, hidden: int = CFG.DIM):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.gru = nn.GRU(input_size=emb_dim, hidden_size=hidden, batch_first=True, bidirectional=True)\n",
    "        self.proj = nn.Linear(hidden * 2, emb_dim)\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, lengths: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.emb(tokens)  # (B, L, E)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        out, h = self.gru(packed)\n",
    "        h_cat = torch.cat([h[-2], h[-1]], dim=-1)  # (B, 2*hidden)\n",
    "        q = self.proj(h_cat)  # (B, D)\n",
    "        return q\n",
    "\n",
    "\n",
    "class TinyCNN(nn.Module):\n",
    "    def __init__(self, out_dim: int = CFG.DIM):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, stride=2, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, out_dim, 3, stride=2, padding=1), nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)  # (B, D, H', W')\n",
    "\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, out_dim: int = CFG.DIM, pretrained: bool = True, img_size: int = CFG.IMG_SIZE):\n",
    "        super().__init__()\n",
    "        self.resize = None\n",
    "        if _BACKBONE_OK:\n",
    "            try:\n",
    "                weights = ResNet18_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "                m = resnet18(weights=weights)\n",
    "                layers = list(m.children())[:-2]  # (B, 512, H/32, W/32)\n",
    "                self.backbone = nn.Sequential(*layers)\n",
    "                self.proj = nn.Conv2d(512, out_dim, 1)\n",
    "                from torchvision import transforms as T\n",
    "                self.resize = T.Compose([T.Resize((img_size, img_size)), T.ToTensor()])\n",
    "            except Exception:\n",
    "                self.backbone = TinyCNN(out_dim)\n",
    "                self.proj = nn.Identity()\n",
    "        else:\n",
    "            self.backbone = TinyCNN(out_dim)\n",
    "            self.proj = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = self.backbone(x)\n",
    "        f = self.proj(f)\n",
    "        return f\n",
    "\n",
    "\n",
    "class CrossAttentionBBox(nn.Module):\n",
    "    def __init__(self, dim: int = CFG.DIM):\n",
    "        super().__init__()\n",
    "        self.q_proj = nn.Linear(dim, dim)\n",
    "        self.k_proj = nn.Conv2d(dim, dim, 1)\n",
    "        self.v_proj = nn.Conv2d(dim, dim, 1)\n",
    "        self.bbox_head = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(dim, 4)  # (cx, cy, w, h) normalized via sigmoid\n",
    "        )\n",
    "\n",
    "    def forward(self, q_vec: torch.Tensor, fmap: torch.Tensor) -> torch.Tensor:\n",
    "        B, D, H, W = fmap.shape\n",
    "        q = self.q_proj(q_vec)             # (B, D)\n",
    "        K = self.k_proj(fmap)              # (B, D, H, W)\n",
    "        V = self.v_proj(fmap)              # (B, D, H, W)\n",
    "\n",
    "        Kf = K.flatten(2).transpose(1, 2)  # (B, HW, D)\n",
    "        Vf = V.flatten(2).transpose(1, 2)  # (B, HW, D)\n",
    "        q = q.unsqueeze(1)                 # (B, 1, D)\n",
    "\n",
    "        attn = torch.matmul(q, Kf.transpose(1, 2)) / math.sqrt(D)  # (B, 1, HW)\n",
    "        attn = torch.softmax(attn, dim=-1)\n",
    "        ctx = torch.matmul(attn, Vf).squeeze(1)  # (B, D)\n",
    "\n",
    "        pred = self.bbox_head(ctx)         # (B, 4)\n",
    "        pred = torch.sigmoid(pred)         # normalize to [0,1]\n",
    "        return pred\n",
    "\n",
    "\n",
    "class CrossAttnVLM(nn.Module):\n",
    "    def __init__(self, vocab_size: int, dim: int = CFG.DIM, pretrained_backbone: bool = True, img_size: int = CFG.IMG_SIZE):\n",
    "        super().__init__()\n",
    "        self.txt = TextEncoder(vocab_size=vocab_size, emb_dim=dim, hidden=dim)\n",
    "        self.img = ImageEncoder(out_dim=dim, pretrained=pretrained_backbone, img_size=img_size)\n",
    "        self.head = CrossAttentionBBox(dim=dim)\n",
    "\n",
    "    def forward(self, images: torch.Tensor, tokens: torch.Tensor, lengths: torch.Tensor) -> torch.Tensor:\n",
    "        q = self.txt(tokens, lengths)             # (B, D)\n",
    "        fmap = self.img(images)                   # (B, D, H', W')\n",
    "        pred_norm = self.head(q, fmap)            # (B, 4) in [0,1]\n",
    "        return pred_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c262d2",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28103be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_xywh_pixel(pred_xywh, gt_xywh):\n",
    "    px, py, pw, ph = pred_xywh\n",
    "    gx, gy, gw, gh = gt_xywh\n",
    "    px2, py2 = px + pw, py + ph\n",
    "    gx2, gy2 = gx + gw, gy + gh\n",
    "    ix1, iy1 = max(px, gx), max(py, gy)\n",
    "    ix2, iy2 = min(px2, gx2), min(py2, gy2)\n",
    "    inter = max(0, ix2 - ix1) * max(0, iy2 - iy1)\n",
    "    union = pw * ph + gw * gh - inter if (pw * ph + gw * gh - inter) > 0 else 1e-6\n",
    "    return inter / union\n",
    "\n",
    "\n",
    "def make_loader(json_dir: str, jpg_dir: str, vocab: Vocab = None, build_vocab: bool = False,\n",
    "                batch_size: int = CFG.BATCH_SIZE, img_size: int = CFG.IMG_SIZE,\n",
    "                num_workers: int = CFG.NUM_WORKERS, shuffle: bool = False):\n",
    "    json_files = find_jsons(json_dir)\n",
    "    ds = UniDSet(json_files, jpg_dir=jpg_dir, vocab=vocab, build_vocab=build_vocab,\n",
    "                 resize_to=(img_size, img_size))\n",
    "    if build_vocab:\n",
    "        # use only supervised samples (have bbox)\n",
    "        sup_idx = [i for i in range(len(ds)) if ds[i][\"target\"] is not None]\n",
    "        if len(sup_idx) == 0:\n",
    "            raise RuntimeError(\"No supervised samples (no bboxes) in given json_dir.\")\n",
    "        ds = torch.utils.data.Subset(ds, sup_idx)\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=shuffle,\n",
    "                    num_workers=num_workers, collate_fn=collate_fn)\n",
    "    return ds, dl\n",
    "\n",
    "\n",
    "def train_loop(args):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # Build dataset + vocab on train jsons\n",
    "    train_ds, train_dl = make_loader(args.json_dir, args.jpg_dir, vocab=None, build_vocab=True,\n",
    "                                     batch_size=args.batch_size, img_size=args.img_size,\n",
    "                                     num_workers=args.num_workers, shuffle=True)\n",
    "\n",
    "    model = CrossAttnVLM(vocab_size=len(train_ds.dataset.vocab.itos) if isinstance(train_ds, torch.utils.data.Subset) else len(train_ds.vocab.itos),\n",
    "                         dim=args.dim, pretrained_backbone=not args.no_pretrain, img_size=args.img_size).to(device)\n",
    "\n",
    "    # Resolve vocab (Subset wrapper case)\n",
    "    vocab = train_ds.dataset.vocab if isinstance(train_ds, torch.utils.data.Subset) else train_ds.vocab\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "    scaler = torch.amp.GradScaler('cuda', enabled=torch.cuda.is_available())\n",
    "\n",
    "    total_samples = len(train_ds)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        model.train()\n",
    "        running = 0.0\n",
    "        for imgs, ids, lens, targets, meta in train_dl:\n",
    "            imgs = imgs.to(device); ids = ids.to(device); lens = lens.to(device)\n",
    "            t = torch.stack([tar for tar in targets if tar is not None], dim=0).to(device)  # (B,4)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with torch.amp.autocast('cuda', enabled=torch.cuda.is_available()):\n",
    "                pred = model(imgs, ids, lens)   # (B,4) normalized\n",
    "                loss = F.smooth_l1_loss(pred, t, reduction=\"mean\")\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            running += float(loss.item()) * imgs.size(0)\n",
    "        scheduler.step()\n",
    "        avg = running / total_samples\n",
    "        print(f\"[Epoch {epoch}/{args.epochs}] loss={avg:.4f}  lr={scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "    os.makedirs(os.path.dirname(args.save_ckpt), exist_ok=True)\n",
    "    torch.save({\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"vocab_itos\": vocab.itos,\n",
    "        \"dim\": args.dim,\n",
    "        \"no_pretrain\": args.no_pretrain,\n",
    "        \"img_size\": args.img_size,\n",
    "    }, args.save_ckpt)\n",
    "    print(f\"[Saved] {args.save_ckpt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c02b08a",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeab13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_model_from_ckpt(ckpt_path: str, device: torch.device):\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    vocab = Vocab(); vocab.itos = ckpt[\"vocab_itos\"]; vocab.stoi = {t: i for i, t in enumerate(vocab.itos)}\n",
    "    model = CrossAttnVLM(vocab_size=len(vocab.itos), dim=ckpt[\"dim\"],\n",
    "                         pretrained_backbone=not ckpt.get(\"no_pretrain\", False),\n",
    "                         img_size=ckpt.get(\"img_size\", CFG.IMG_SIZE)).to(device)\n",
    "    model.load_state_dict(ckpt[\"model_state\"]); model.eval()\n",
    "    img_size = ckpt.get(\"img_size\", CFG.IMG_SIZE)\n",
    "    return model, vocab, img_size\n",
    "\n",
    "\n",
    "def evaluate_loop(args):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model, vocab, img_size = _load_model_from_ckpt(args.ckpt, device)\n",
    "\n",
    "    json_files = find_jsons(args.json_dir)\n",
    "    ds = UniDSet(json_files, jpg_dir=args.jpg_dir, vocab=vocab, build_vocab=False,\n",
    "                 resize_to=(img_size, img_size))\n",
    "    dl = DataLoader(ds, batch_size=args.batch_size, shuffle=False,\n",
    "                    num_workers=args.num_workers, collate_fn=collate_fn)\n",
    "\n",
    "    rows = []; ious = []\n",
    "    with torch.no_grad():\n",
    "        for imgs, ids, lens, targets, meta in dl:\n",
    "            imgs = imgs.to(device); ids = ids.to(device); lens = lens.to(device)\n",
    "            pred = model(imgs, ids, lens)  # (B,4) normalized\n",
    "            for i in range(imgs.size(0)):\n",
    "                W, H = meta[i][\"orig_size\"]\n",
    "                cx, cy, nw, nh = [float(v) for v in pred[i].cpu().numpy().tolist()]\n",
    "                x = (cx - nw / 2.0) * W; y = (cy - nh / 2.0) * H\n",
    "                w = nw * W; h = nh * H\n",
    "                rows.append({\n",
    "                    \"query_id\": meta[i][\"query_id\"], \"query_text\": meta[i][\"query_text\"],\n",
    "                    \"pred_x\": x, \"pred_y\": y, \"pred_w\": w, \"pred_h\": h\n",
    "                })\n",
    "                if targets[i] is not None:\n",
    "                    gt = [float(v) for v in targets[i].numpy().tolist()]\n",
    "                    gx = (gt[0] - gt[2] / 2.0) * W; gy = (gt[1] - gt[3] / 2.0) * H\n",
    "                    gw = gt[2] * W; gh = gt[3] * H\n",
    "                    ious.append(iou_xywh_pixel([x, y, w, h], [gx, gy, gw, gh]))\n",
    "\n",
    "    os.makedirs(os.path.dirname(args.out_csv), exist_ok=True)\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(rows, columns=[\"query_id\", \"query_text\", \"pred_x\", \"pred_y\", \"pred_w\", \"pred_h\"])\n",
    "    df.to_csv(args.out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[Saved] {args.out_csv}\")\n",
    "    if ious:\n",
    "        print(f\"[Eval] mIoU={float(np.mean(ious))}\")\n",
    "    else:\n",
    "        print(\"[Eval] No GT found; mIoU not computed.\")\n",
    "\n",
    "\n",
    "def predict_loop(args):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model, vocab, img_size = _load_model_from_ckpt(args.ckpt, device)\n",
    "\n",
    "    json_files = find_jsons(args.json_dir)\n",
    "    ds = UniDSet(json_files, jpg_dir=args.jpg_dir, vocab=vocab, build_vocab=False,\n",
    "                 resize_to=(img_size, img_size))\n",
    "    dl = DataLoader(ds, batch_size=args.batch_size, shuffle=False,\n",
    "                    num_workers=args.num_workers, collate_fn=collate_fn)\n",
    "\n",
    "    rows = []\n",
    "    with torch.no_grad():\n",
    "        for imgs, ids, lens, targets, meta in dl:\n",
    "            imgs = imgs.to(device); ids = ids.to(device); lens = lens.to(device)\n",
    "            pred = model(imgs, ids, lens)  # (B,4) normalized\n",
    "            for i in range(imgs.size(0)):\n",
    "                W, H = meta[i][\"orig_size\"]\n",
    "                cx, cy, nw, nh = [float(v) for v in pred[i].cpu().numpy().tolist()]\n",
    "                x = (cx - nw / 2.0) * W; y = (cy - nh / 2.0) * H\n",
    "                w = nw * W; h = nh * H\n",
    "                rows.append({\n",
    "                    \"query_id\": meta[i][\"query_id\"], \"query_text\": meta[i][\"query_text\"],\n",
    "                    \"pred_x\": x, \"pred_y\": y, \"pred_w\": w, \"pred_h\": h\n",
    "                })\n",
    "\n",
    "    os.makedirs(os.path.dirname(args.out_csv), exist_ok=True)\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(rows, columns=[\"query_id\", \"query_text\", \"pred_x\", \"pred_y\", \"pred_w\", \"pred_h\"])\n",
    "    df.to_csv(args.out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[Saved] {args.out_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339d609c",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502c7c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_submission(csv_path: str, zip_path: str):\n",
    "    os.makedirs(os.path.dirname(zip_path), exist_ok=True)\n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "        arcname = os.path.basename(csv_path)\n",
    "        zf.write(csv_path, arcname=arcname)\n",
    "    print(f\"[Submission] Zipped {csv_path} → {zip_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bb3a87",
   "metadata": {},
   "source": [
    "# CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becdb64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    ap = argparse.ArgumentParser()\n",
    "    sub = ap.add_subparsers(dest=\"cmd\", required=True)\n",
    "\n",
    "    def add_common(p):\n",
    "        p.add_argument(\"--json_dir\", type=str, default=CFG.JSON_DIR, help=\"Directory with JSON files\")\n",
    "        p.add_argument(\"--jpg_dir\", type=str, default=CFG.JPG_DIR, help=\"Directory with JPG images\")\n",
    "        p.add_argument(\"--batch_size\", type=int, default=CFG.BATCH_SIZE)\n",
    "        p.add_argument(\"--img_size\", type=int, default=CFG.IMG_SIZE)\n",
    "        p.add_argument(\"--dim\", type=int, default=CFG.DIM)\n",
    "        p.add_argument(\"--num_workers\", type=int, default=CFG.NUM_WORKERS)\n",
    "\n",
    "    # train\n",
    "    p_train = sub.add_parser(\"train\")\n",
    "    add_common(p_train)\n",
    "    p_train.add_argument(\"--epochs\", type=int, default=CFG.EPOCHS)\n",
    "    p_train.add_argument(\"--lr\", type=float, default=CFG.LEARNING_RATE)\n",
    "    p_train.add_argument(\"--no_pretrain\", action=\"store_true\", help=\"Disable ImageNet pretrained weights\")\n",
    "    p_train.add_argument(\"--save_ckpt\", type=str, default=CFG.CKPT_PATH)\n",
    "\n",
    "    # eval\n",
    "    p_eval = sub.add_parser(\"eval\")\n",
    "    add_common(p_eval)\n",
    "    p_eval.add_argument(\"--ckpt\", type=str, required=True)\n",
    "    p_eval.add_argument(\"--out_csv\", type=str, default=CFG.EVAL_CSV)\n",
    "\n",
    "    # predict\n",
    "    p_pred = sub.add_parser(\"predict\")\n",
    "    add_common(p_pred)\n",
    "    p_pred.add_argument(\"--ckpt\", type=str, required=True)\n",
    "    p_pred.add_argument(\"--out_csv\", type=str, default=CFG.PRED_CSV)\n",
    "\n",
    "    # submission (zip any csv)\n",
    "    p_zip = sub.add_parser(\"zip\")\n",
    "    p_zip.add_argument(\"--csv\", type=str, required=True)\n",
    "    p_zip.add_argument(\"--out_zip\", type=str, default=CFG.SUBMISSION_ZIP)\n",
    "\n",
    "    return ap.parse_args()\n",
    "\n",
    "\n",
    "def main():\n",
    "    seed_everything(CFG.SEED)\n",
    "    args = get_args()\n",
    "\n",
    "    if args.cmd == \"train\":\n",
    "        train_loop(args)\n",
    "    elif args.cmd == \"eval\":\n",
    "        evaluate_loop(args)\n",
    "    elif args.cmd == \"predict\":\n",
    "        predict_loop(args)\n",
    "    elif args.cmd == \"zip\":\n",
    "        zip_submission(args.csv, args.out_zip)\n",
    "    else:\n",
    "        raise ValueError(args.cmd)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
